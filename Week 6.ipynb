{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e2d4a6-dc65-4ab5-b3e3-9a3871e08c6d",
   "metadata": {},
   "source": [
    "# COMP-2704: Supervised Machine Learning\n",
    "### <span style=\"color:blue\"> Week 6 </span>\n",
    "\n",
    "## <span style=\"color:blue\"> Chapter 4 continued</span>\n",
    "### Another alternative to avoiding overfitting: Regularization\n",
    "\n",
    "* Regularization is a method for lowering the amount of overfitting.\n",
    "* One can tune the hyperparameters of a model to overfit slightly, then use regularization to improve the model.\n",
    "* This will tend to increase the training error, but lower the validation error.\n",
    "* Remember, one should not find the testing error until after all of the hyperparameters, including those for regularization, have been set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0fd9a-f3dc-48f2-ab8e-16e301cc1d46",
   "metadata": {},
   "source": [
    "### The leaky roof example\n",
    "\n",
    "<img src='leaky_house.jpg' width='300'/>\n",
    "\n",
    "*Generated by AI*\n",
    "\n",
    "There are three roofers to consider. Here is how well they perform:\n",
    "* Roofer 1: uses cardboard and duct tape; reduces leakager to 1,000 ml of water per day.\n",
    "* Roofer 2: uses shingles and nails; reduces leakage to 1 ml of water per day.\n",
    "* Roofer 3: uses marble slabs and reinforcing beams; reduces leakage to 0 ml of water per day. \n",
    "\n",
    "A repair by roofer 3 will work the best. But how expensive is each fix?\n",
    "* Roofer 1: \\$1\n",
    "* Roofer 2: \\$100\n",
    "* Roofer 3: \\$100,000\n",
    "\n",
    "Roofer 1 is the cheapest. However, roofer 2 seems like the best overall option.\n",
    "\n",
    "**<span style=\"color:green\">Q: What is a metric that would measure roofer 2 to be the best?</span>**\n",
    "\n",
    "Well, we could add the *performance* and the *price*, then choose the roofer with the lowest total value:\n",
    "* Roofer 1: $1000 + 1 = 1001$\n",
    "* Roofer 2: $100 + 1 = 101$\n",
    "* Roofer 3: $100,000 + 1 = 100,001$\n",
    "\n",
    "We see that roofer 2 has the lowest value of this metric.\n",
    "\n",
    "* In machine learning, *performance* is analogous to *error* and *price* is analogous to a *regularization* term.\n",
    "* Next we discuss how to create a regularization term and how it is combined with error to create a new error function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607819d-4e8e-4d05-8a35-72b084ddef1b",
   "metadata": {},
   "source": [
    "### Another example of overfitting: Movie recommendations\n",
    "\n",
    "Consider the following example:\n",
    "* We have 10 movies, M1, M2, ... , M10, each is a feature.\n",
    "* There are 100 users, each is a sample of data.\n",
    "* The data set has 100 rows; each column has the time $x_i$ (in seconds) that the user watched movie $i$.\n",
    "* We want a model that predicts the time a user will spend watching a new movie, M11.\n",
    "* We develop the following two models:\n",
    "    * Model 1: $\\hat{y} = 2x_3 + 1.4x_7 – 0.5x_7 + 8$\n",
    "    * Model 2: $\\hat{y} = 22x_1 – 103x_2 – 14x_3 + 109x_4 – 93x_5 + 203x_6 + 87x_7 – 55x_8 + 378x_9 – 25x_{10}+8$\n",
    "    \n",
    "**<span style=\"color:green\">Q: Which model do you think is overfitting?</span>**\n",
    "\n",
    "* Notice that many coefficients in model 1 are 0.\n",
    "* There are more coefficients in model 2, and they have larger absolute values.\n",
    "* Using only regression error (such as RMSE) will lead to a model like model 2 during training. We need a new error function that leads to something more like model 1.\n",
    "\n",
    "To create a new error function, consider the following two polynomial 'norms':\n",
    "* **L1** norm: sum the absolue values of all coefficients (but not the bias):\n",
    "    * Model 1 $\\Rightarrow |2| + |1.4| + |-0.5| = 3.9$\n",
    "    * Model 2 $\\Rightarrow |22| + |–103| + |–14| + |109| + |–93| + |203| + |87| + |–55| + |378| + |–25| = 1,089$\n",
    "* **L2** norm: sum the squared values of all coefficients (but not the bias):\n",
    "    * Model 1 $\\Rightarrow 2^2 + 1.4^2 + (–0.5)^2 = 6.21$\n",
    "    * Model 2 $\\Rightarrow 22^2 + (–103)^2 + (–14)^2 + 109^2 + (-93)^2 + 203^2 + 87^2 + (–55)^2 + 378^2 + (–25)^2 = 227,131$\n",
    "    \n",
    "So we see that these norms are larger for complex models that tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57e57d-6b6d-4516-b992-e5c7b85acf9d",
   "metadata": {},
   "source": [
    "### Modifying the error function to solve our problem: Lasso regression and ridge regression\n",
    "\n",
    "We now have regression error and regularization term (L1 or L2), and we would like to optimize both:\n",
    "* Keeping the regression error low will lead to better predictions on the training set.\n",
    "* Keeping the norm low will lead prevent the model from overfitting so it makes good predictions on new data.\n",
    "\n",
    "The way to do this is to create a new error function from the sum of these two terms:\n",
    "$$ \\text{error} = \\text{regression error} + \\lambda \\left(\\text{regularization term}\\right)$$\n",
    "* $\\lambda$ is a hyperparameter that controls the relative strength of each term.\n",
    "    * making $\\lambda > 1$ makes the regularization term dominate over the error, and training will focus more on preventing overfitting.\n",
    "    * making $\\lambda < 1$ makes the regression error dominant, and training will focus more on making better predictions.\n",
    "    \n",
    "There are three ways to add regularization:\n",
    "* **Lasso** regularization uses the L1 norm\n",
    "    * Coefficients tend to shrink to zero: $2 → 1.99 → 1.98 → … → 0.02 → 0.01 → 0$\n",
    "    * Leaves a model with fewer coefficients.\n",
    "* **Ridge** regularization uses the L2 norm\n",
    "    * Coefficients tend to become smaller, but not zero: $2 → 1.98 → 1.9602 → … → 0.2734 → 0.2707 → 0.2680$\n",
    "    * Leaves a model with more coefficients than Lasso regularization, but they are small.\n",
    "* **Elastic net** regularization uses both the L1 and L2 norms\n",
    "    * First uses ridge regularization, then lasso.\n",
    "    * The $\\lambda_1$ and $\\lambda_2$ coefficients for each can be set separately.\n",
    "    \n",
    "<img src='Fig4.7.png' width='600'/>\n",
    "\n",
    "<span style=\"color:red\">*Let us now review the textbook code Polynomial_regression_regularization.ipynb.*</span>\n",
    "\n",
    "**<span style=\"color:green\">Q: Notice that we are using a linear regression model to do polynomial regression. Can you explain that?</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc99b2b-581f-4ace-859b-68ad073fd336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
